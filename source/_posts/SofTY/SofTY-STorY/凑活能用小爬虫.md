---

title:  SofTY vol.2|凑活能用小爬虫
date: 2023-04-14 22:25:47
cover: https://i.imgloc.com/2023/04/15/i7gSdZ.png
tags:
    - Python
    - SofTY
    - 原创
categories:
    - [SofTY, SofTY-STorY]
---

![i7gSdZ.png](https://i.imgloc.com/2023/04/15/i7gSdZ.png)
<!--more-->
在开发中，我们时不时就发现自己要从某网站里扣出点有用的信息 ~~（比如cosplay套图和一堆地狱笑话）~~

额，暴露了……但是确实我们会用到爬虫，对吧？

可是吧……爬虫的一个显著的特点是代码会稍稍多一点（当然是Python啦），导致有些时候我们会懒得为了一个简简单单的小目的而专门造一个爬虫。

额……这又是STY的问题，可是爬虫的格式相对还是很固定的，所以咱开启正题

{% raw %}
<center><font size = 10>如何快速造一个凑合能用的简单小爬虫</font></center>
{% endraw %}

# STEP 1 - “爬”虫
某非著名编程爱好者曾说过：
> 爬虫要自己爬到互联网上。
> 
> ——浅野ナツキ

所以我们第一步要让这个Python文件能++访问网站++。

为了让它能访问网站，我们需要一个`requests`模块啊！

~~~Python
import requests

headers = {'useragent':'xxxxxxxxxx'}
url = 'https://ameitsu.github.io'
res = requests.get(url=url,headers=headers)

~~~

第3行里的headers很神奇——它是一个++小区门禁卡++一样的东西。

从设计目的上来说，useragent是服务器判断访问文件的是人还是爬虫的防护措施，重点在于反爬虫。但是这也是它最大的问题：光天化日之下的爬虫能防住，++套了个马甲++的爬虫就不行了

只要headers部分换上一个货真价实的，从浏览器里copy的useragent，服务器就不知道这到底是爬虫还是人了。（妈耶，STY小区门口的门禁系统比这个严密了不知多少倍）

如果你懒得去查自己浏览器的useragent，那不如试试`fake_useragent`模块，可以生成一个以假乱真的header

第5行里的`res.get()`是本部分的核心科技。`url`变量是将要访问的网址，有了网址和headers`res.get()`就能假扮人类访问目标网页了。
![i7goqF.png](https://i.imgloc.com/2023/04/15/i7goqF.png)

# STEP 2 - 处理战利品
人类上网会直接找到关键信息，然后[Crtl]{.kbd}+[C]{.kbd},相当于信息的查看，筛选和利用一步就完成了。但是吧爬虫它不长脑子，得分几步做。

## a.处理.html文件
爬虫的网络模块访问网址后只会把网页的html文档copy下来。真正分析出信息还得在本地进行。

所以我们需要的是一个`BeautifulSoup`啊！把网页的html扣下来后像这样整就能正常地把html转换成方便解析的亚子。

~~~python
from bs4 import BeautifulSoup
import requests

headers = {'useragent':'xxxxxxxxxx'}
url = 'https://ameitsu.github.io'
res = requests.get(url=url,headers=headers)
soup =BeautifulSoup(res.text,"lxml")
~~~

`soup`在这里就是能用来解析的html字符串。根据STY目前的经验，这个变量叫"soup"的原因纯粹是++这个模块叫"BeautifulSoup"++……

## b.找标签
BeautifulSoup的大杀器是`.findall()`,可以让你一下就通过html相关标签的规律直接匹配标签。
~~~python
 contents = soup.findall(_target = '',_class = '')
~~~
以上纯属示例，具体该匹配的的属性和值得看真实的需要。
![i7gSdZ.png](https://i.imgloc.com/2023/04/15/i7gSdZ.png)
最后输出的是一个列表类型的变量，看起来不好康，所以……


# STEP 3-输出
……就用循环的方式打印一下。
~~~python
for content in contents:
    print(content.text)
~~~
但是！如果需要记录信息的话还是写入到一个文件里更安全。
~~~python
file = open('result.txt',encoding = 'UTF-8')
for content in contents:
    file.write(content.text) 
~~~

就酱。现在你有一个简单粗暴凑合能用的小爬虫了

大概的代码是这样：
~~~python
from bs4 import BeautifulSoup
import requests

headers = {'useragent':'xxxxxxxxxx'}
url = 'https://www.example.com'
res = requests.get(url=url,headers=headers)
soup =BeautifulSoup(res.text,"lxml")
contents = soup.findall(_target = '',_class = '')

file = open('result.txt',encoding = 'UTF-8')
for content in contents:
    file.write(content.text) 
~~~


啊？你问怎么抓包下载视频？BeautifulSoup模块还有什么神奇操作？requests模块还能玩出什么花？

那STY只能说……
![i7NB8U.jpeg](https://i.imgloc.com/2023/04/15/i7NB8U.jpeg)

# 最后

中考一模快到了，我向孔子许愿。

我祈福说让他保佑我7科满分，孔子说不行，只能三科。

我说好的，语言学科、科学学科、素养学科。

孔子说不行，只能两科。

我说好的，文科、理科。

孔子说不行，只能一科。

我说可以。

孔子一脸茫然地看着我，问：‘哪一科？’

我说：‘每一科’

:::info
向名人许愿 !!并不!! 能提高您的一模成绩
:::




